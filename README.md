This project investigates the effectiveness of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, both en- hanced with attention mechanisms, for the task of text summarization. Utilizing the ex- tensive and diverse WikiHow dataset, which comprises over 230,000 article-summary pairs, this project aims to enhance the adaptability of sequence-to-sequence (Seq2Seq) models to the varied lengths and styles of texts typical in everyday applications. By focusing on quan- titative metrics such as ROUGE scores, the project highlights the strengths and limitations of each model in managing the complex struc- ture of language and producing concise, rele- vant summaries from longer texts. Preliminary results suggest that while GRU models train more rapidly, the LSTM models demonstrate superior performance in handling long-range dependencies, a crucial aspect of generating coherent and comprehensive summaries.
